{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsCMBPbmLdyHw/uJd5QBy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuhinkm/MAS-Agentic-AI/blob/main/Strmlit_UI_Gemini_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "We shall the the functions from Optimization Agentic Tools - Comparison of Gemini and OpenAI.ipynb file and convert into a strmlit project"
      ],
      "metadata": {
        "id": "ImX00y5Xc1Lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "5GK9KY6faYSM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community --quiet\n",
        "!pip install langchain_groq --quiet\n",
        "!pip install langchain-google-genai --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!apt-get install -y -qq coinor-cbc --quiet\n",
        "!pip install pyomo --quiet\n",
        "!pip install streamlit --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "import langchain_community\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import math\n",
        "import os\n",
        "from geopy.geocoders import Nominatim\n",
        "from langchain_core.messages import AIMessage,HumanMessage\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from google.colab import userdata\n",
        "from langchain.agents import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pyomo.environ import *\n",
        "import pyomo\n",
        "import random\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "\n",
        "# input dictionary\n",
        "input_dict = {}\n",
        "random.seed(33)\n",
        "for i in range(1,25):\n",
        "  input_dict['hour_'+str(i)]= random.randint(23,79)\n",
        "\n",
        "# Model setting\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_TOKEN')\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "def optimizer_run(human_query, input_dict, llm):\n",
        "    \"\"\"\n",
        "    This function simulates your agent's response. This function takes another two functions and perform\n",
        "    calculations intended by the users\n",
        "    \"\"\"\n",
        "\n",
        "    @tool\n",
        "    def get_dict_values(input_dict:dict, high_percentile:int, low_percentile:int=10):\n",
        "      '''\n",
        "      This function takes dictionary as input and return defined percentile ranges value of a dictionary\n",
        "      '''\n",
        "      # Extract the values from the dictionary\n",
        "      values = list(input_dict.values())\n",
        "\n",
        "      # Calculate the 75th percentile\n",
        "      high_val = np.percentile(values, high_percentile)\n",
        "      low_val = np.percentile(values, low_percentile)\n",
        "\n",
        "      return (high_val, low_val)\n",
        "\n",
        "    @tool\n",
        "    def sample_optimization(high_val:float, low_val:float):\n",
        "      '''\n",
        "      This is an optimization function which will take two integer inputs, perform the necessary\n",
        "      calculations and provide float output\n",
        "      '''\n",
        "      model = ConcreteModel()\n",
        "\n",
        "      # Define variables\n",
        "      model.x = Var(within=NonNegativeReals)\n",
        "      model.y = Var(within=NonNegativeReals)\n",
        "\n",
        "      # Define objective function: Minimize x + y\n",
        "      model.objective = Objective(expr=model.x + model.y, sense=minimize)\n",
        "\n",
        "      # Define constraints\n",
        "      model.constraint1 = Constraint(expr=model.x + 2*model.y >= int(high_val))\n",
        "      model.constraint2 = Constraint(expr=model.x - model.y <= int(low_val))\n",
        "\n",
        "      # Solve the model using GLPK solver\n",
        "      solver = pyomo.environ.SolverFactory('cbc')\n",
        "      # solver = pyomo.environ.SolverFactory('gurobi')\n",
        "      solver.solve(model)\n",
        "\n",
        "      return (value(model.objective))\n",
        "\n",
        "    prompt_gemini = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an agentic AI solution. You have 2 functions which you need to execute as per human input prompt. The first function is 'get_dict_values' function which will take an input dictionary and then two integer values from human prompt, which are called high percentile value and low percentile value. Use the dictionary provided in the 'input_dict' variable for this function. In case any of the two numbers are input anything other than integer, please return a message as 'You should provide only integer values as high percentile and low percentile values. Please modify the prompt' and complete the chain. For valid values of high percentile and low percentile, execute 'get_dict_values' and generate high_val and low_val parameters. Use these parameters as input to 'sample_optimization' function and generate the final output.\"),\n",
        "        (\"human\", \"{input}\\n\\nHere is the dictionary to use:\\n{input_dict}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    tools = [get_dict_values, sample_optimization]\n",
        "    agent = create_tool_calling_agent(llm, tools, prompt_gemini)\n",
        "\n",
        "    # run\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    result = agent_executor.invoke({\"input\":  human_query, \"input_dict\": input_dict})\n",
        "    return(result[\"output\"])\n",
        "\n",
        "st.title(\"Streamlit Chatbot with Agent Integration\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"Ask me anything...\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Get agent's response\n",
        "    agent_response = optimizer_run(prompt, input_dict, llm)\n",
        "\n",
        "    # Add agent's response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": agent_response})\n",
        "    # Display agent's response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(agent_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihZqEJgalGlf",
        "outputId": "4b6378cc-fa1e-420e-c1f2-f0146e642b18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install localtunnel"
      ],
      "metadata": {
        "id": "wOACoOlal9Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!npm audit fix --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZThF-uwl0YV",
        "outputId": "73b071bd-f2c2-4ecf-830e-9d4f6c05cf97",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 890ms\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94musing --force\u001b[39m Recommended protections disabled.\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94maudit\u001b[39m Updating localtunnel to 1.8.3, which is a SemVer major change.\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m cryptiles@2.0.5: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m boom@2.10.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m sntp@1.0.9: This module moved to @hapi/sntp. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m uuid@3.4.0: Please upgrade  to version 7 or higher.  Older versions may use Math.random() in certain circumstances, which is known to be problematic.  See https://v8.dev/blog/math-random for details.\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m hoek@2.16.3: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m request@2.81.0: request has been deprecated, see https://github.com/request/request/issues/3142\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m har-validator@4.2.1: this library is no longer supported\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m hawk@3.1.3: This module moved to @hapi/hawk. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "added 83 packages, removed 10 packages, changed 11 packages, and audited 96 packages in 8s\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K11 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "\u001b[1m# npm audit report\u001b[22m\n",
            "\n",
            "\u001b[1majv\u001b[22m  <6.12.3\n",
            "Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n",
            "\u001b[1mPrototype Pollution in Ajv\u001b[22m - https://github.com/advisories/GHSA-v88g-cgmw-v5xw\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/ajv\u001b[22m\n",
            "  \u001b[1mhar-validator\u001b[22m  3.3.0 - 5.1.0\n",
            "  Depends on vulnerable versions of \u001b[1majv\u001b[22m\n",
            "  \u001b[2mnode_modules/har-validator\u001b[22m\n",
            "    \u001b[1mrequest\u001b[22m  *\n",
            "    Depends on vulnerable versions of \u001b[1mform-data\u001b[22m\n",
            "    Depends on vulnerable versions of \u001b[1mhar-validator\u001b[22m\n",
            "    Depends on vulnerable versions of \u001b[1mhawk\u001b[22m\n",
            "    Depends on vulnerable versions of \u001b[1mtough-cookie\u001b[22m\n",
            "    \u001b[2mnode_modules/request\u001b[22m\n",
            "      \u001b[1mlocaltunnel\u001b[22m  <=1.9.0\n",
            "      Depends on vulnerable versions of \u001b[1mdebug\u001b[22m\n",
            "      Depends on vulnerable versions of \u001b[1mrequest\u001b[22m\n",
            "      \u001b[2mnode_modules/localtunnel\u001b[22m\n",
            "\n",
            "\u001b[1mdebug\u001b[22m  <=2.6.8\n",
            "Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n",
            "\u001b[1mdebug Inefficient Regular Expression Complexity vulnerability\u001b[22m - https://github.com/advisories/GHSA-9vvw-cc9w-f27h\n",
            "\u001b[1mRegular Expression Denial of Service in debug\u001b[22m - https://github.com/advisories/GHSA-gxpj-cx7g-858c\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/debug\u001b[22m\n",
            "\n",
            "\u001b[1mform-data\u001b[22m  <2.5.4\n",
            "Severity: \u001b[35m\u001b[1mcritical\u001b[22m\u001b[39m\n",
            "\u001b[1mform-data uses unsafe random function in form-data for choosing boundary\u001b[22m - https://github.com/advisories/GHSA-fjxv-7rqg-78g4\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/form-data\u001b[22m\n",
            "\n",
            "\u001b[1mhawk\u001b[22m  <=9.0.0\n",
            "Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n",
            "\u001b[1mUncontrolled Resource Consumption in Hawk\u001b[22m - https://github.com/advisories/GHSA-44pw-h2cw-w3vq\n",
            "Depends on vulnerable versions of \u001b[1mboom\u001b[22m\n",
            "Depends on vulnerable versions of \u001b[1mcryptiles\u001b[22m\n",
            "Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n",
            "Depends on vulnerable versions of \u001b[1msntp\u001b[22m\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/hawk\u001b[22m\n",
            "\n",
            "\u001b[1mhoek\u001b[22m  *\n",
            "Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n",
            "\u001b[1mhoek subject to prototype pollution via the clone function.\u001b[22m - https://github.com/advisories/GHSA-c429-5p7v-vgjp\n",
            "\u001b[1mPrototype Pollution in hoek\u001b[22m - https://github.com/advisories/GHSA-jp4x-w63m-7wgm\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/hoek\u001b[22m\n",
            "  \u001b[1mboom\u001b[22m  *\n",
            "  Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n",
            "  \u001b[2mnode_modules/boom\u001b[22m\n",
            "    \u001b[1mcryptiles\u001b[22m  *\n",
            "    Depends on vulnerable versions of \u001b[1mboom\u001b[22m\n",
            "    \u001b[2mnode_modules/cryptiles\u001b[22m\n",
            "  \u001b[1msntp\u001b[22m  0.0.0 || >=0.1.1\n",
            "  Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n",
            "  \u001b[2mnode_modules/sntp\u001b[22m\n",
            "\n",
            "\n",
            "\u001b[1mtough-cookie\u001b[22m  <4.1.3\n",
            "Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n",
            "\u001b[1mtough-cookie Prototype Pollution vulnerability\u001b[22m - https://github.com/advisories/GHSA-72xf-g2v4-qvf3\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/tough-cookie\u001b[22m\n",
            "\n",
            "\u001b[31m\u001b[1m12\u001b[22m\u001b[39m vulnerabilities (3 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m, 7 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m, 2 \u001b[35m\u001b[1mcritical\u001b[22m\u001b[39m)\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run streamlit in background"
      ],
      "metadata": {
        "id": "5Tp6gIwFl_hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "8CekY585mEpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c84118f6-ba51-42f2-8469-dd424ab64ad9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.63.116.10\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0Kyour url is: https://gold-points-prove.loca.lt\n"
          ]
        }
      ]
    }
  ]
}