{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd1TZ+lavTinl5Yz2BNvIU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "In this file, we are trying to create an agentic ai solution with custom optimization function. First, there is one agent which will analyze some percentile values as highest and lowest ranges from a 24 hours distribution and then pass the maximum and minimum value to another agent which will generate the maximum value of the  objective function."
      ],
      "metadata": {
        "id": "EzS4ra2AUKt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community --quiet\n",
        "!pip install langchain_groq --quiet\n",
        "!pip install langchain-google-genai --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!apt-get install -y -qq coinor-cbc --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0vzR8gMYYs4H",
        "outputId": "61819323-65fc-4313-e711-ea1d3cd6478f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSelecting previously unselected package coinor-libcoinutils3v5:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../0-coinor-libcoinutils3v5_2.11.4+repack1-2_amd64.deb ...\n",
            "Unpacking coinor-libcoinutils3v5:amd64 (2.11.4+repack1-2) ...\n",
            "Selecting previously unselected package coinor-libosi1v5:amd64.\n",
            "Preparing to unpack .../1-coinor-libosi1v5_0.108.6+repack1-2_amd64.deb ...\n",
            "Unpacking coinor-libosi1v5:amd64 (0.108.6+repack1-2) ...\n",
            "Selecting previously unselected package coinor-libclp1.\n",
            "Preparing to unpack .../2-coinor-libclp1_1.17.5+repack1-1_amd64.deb ...\n",
            "Unpacking coinor-libclp1 (1.17.5+repack1-1) ...\n",
            "Selecting previously unselected package coinor-libcgl1:amd64.\n",
            "Preparing to unpack .../3-coinor-libcgl1_0.60.3+repack1-3_amd64.deb ...\n",
            "Unpacking coinor-libcgl1:amd64 (0.60.3+repack1-3) ...\n",
            "Selecting previously unselected package coinor-libcbc3:amd64.\n",
            "Preparing to unpack .../4-coinor-libcbc3_2.10.7+ds1-1_amd64.deb ...\n",
            "Unpacking coinor-libcbc3:amd64 (2.10.7+ds1-1) ...\n",
            "Selecting previously unselected package coinor-cbc.\n",
            "Preparing to unpack .../5-coinor-cbc_2.10.7+ds1-1_amd64.deb ...\n",
            "Unpacking coinor-cbc (2.10.7+ds1-1) ...\n",
            "Setting up coinor-libcoinutils3v5:amd64 (2.11.4+repack1-2) ...\n",
            "Setting up coinor-libosi1v5:amd64 (0.108.6+repack1-2) ...\n",
            "Setting up coinor-libclp1 (1.17.5+repack1-1) ...\n",
            "Setting up coinor-libcgl1:amd64 (0.60.3+repack1-3) ...\n",
            "Setting up coinor-libcbc3:amd64 (2.10.7+ds1-1) ...\n",
            "Setting up coinor-cbc (2.10.7+ds1-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "import langchain_community\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import math\n",
        "import os\n",
        "from geopy.geocoders import Nominatim\n",
        "from langchain_core.messages import AIMessage,HumanMessage\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from google.colab import userdata\n",
        "from langchain.agents import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pyomo.environ import *\n",
        "import pyomo\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ols515o7ZLB0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of functions\n",
        "\n",
        "@tool\n",
        "def get_dict_values(input_dict:dict, high_percentile:int, low_percentile:int=10):\n",
        "  '''\n",
        "  This function takes dictionary as input and return defined percentile ranges value of a dictionary\n",
        "  '''\n",
        "  # Extract the values from the dictionary\n",
        "  values = list(input_dict.values())\n",
        "\n",
        "  # Calculate the 75th percentile\n",
        "  high_val = np.percentile(values, high_percentile)\n",
        "  low_val = np.percentile(values, low_percentile)\n",
        "\n",
        "  return (high_val, low_val)\n",
        "\n",
        "@tool\n",
        "def sample_optimization(high_val:float, low_val:float):\n",
        "  '''\n",
        "  This is an optimization function which will take two integer inputs, perform the necessary\n",
        "  calculations and provide float output\n",
        "  '''\n",
        "  model = ConcreteModel()\n",
        "\n",
        "  # Define variables\n",
        "  model.x = Var(within=NonNegativeReals)\n",
        "  model.y = Var(within=NonNegativeReals)\n",
        "\n",
        "  # Define objective function: Minimize x + y\n",
        "  model.objective = Objective(expr=model.x + model.y, sense=minimize)\n",
        "\n",
        "  # Define constraints\n",
        "  model.constraint1 = Constraint(expr=model.x + 2*model.y >= int(high_val))\n",
        "  model.constraint2 = Constraint(expr=model.x - model.y <= int(low_val))\n",
        "\n",
        "  # Solve the model using GLPK solver\n",
        "  solver = pyomo.environ.SolverFactory('cbc')\n",
        "  # solver = pyomo.environ.SolverFactory('gurobi')\n",
        "  solver.solve(model)\n",
        "\n",
        "  return (value(model.objective))"
      ],
      "metadata": {
        "id": "WAI3ZgQ4Zcdj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input dictionary\n",
        "input_dict = {}\n",
        "random.seed(33)\n",
        "for i in range(1,25):\n",
        "  input_dict['hour_'+str(i)]= random.randint(23,79)"
      ],
      "metadata": {
        "id": "jckQ1eRY_lyc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_openai = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an agentic AI solution. You have 2 functions which you need to execute as per human input prompt. The first function is 'get_dict_values_wrapper' which takes two integers: high percentile and low percentile. Use the dictionary already provided in the environment. For valid integer percentiles, call get_dict_values_wrapper and generate high_val and low_val parameters. Use these as inputs to 'sample_optimization' function and produce final output.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_gemini = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an agentic AI solution. You have 2 functions which you need to execute as per human input prompt. The first function is 'get_dict_values' function which will take an input dictionary and then two integer values from human prompt, which are called high percentile value and low percentile value. Use the dictionary provided in the 'input_dict' variable for this function. In case any of the two numbers are input anything other than integer, please return a message as 'You should provide only integer values as high percentile and low percentile values. Please modify the prompt' and complete the chain. For valid values of high percentile and low percentile, execute 'get_dict_values' and generate high_val and low_val parameters. Use these parameters as input to 'sample_optimization' function and generate the final output.\"),\n",
        "        (\"human\", \"{input}\\n\\nHere is the dictionary to use:\\n{input_dict}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "QtfOqA-GAi6r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test run for OpenAI Agent Information passdown**"
      ],
      "metadata": {
        "id": "19bBDDLbIYlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For OpenAI agentic approach with Lngchain, the tool definitation will change. We shall prepare a wrapper to pass the pre-defined data and then call the wrapper function during human prompting"
      ],
      "metadata": {
        "id": "j1QMzIZQNVvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict_values(input_dict:dict, high_percentile:int, low_percentile:int):\n",
        "      '''\n",
        "      This function takes dictionary as input and return defined percentile ranges value of a dictionary\n",
        "      '''\n",
        "      # Extract the values from the dictionary\n",
        "      values = list(input_dict.values())\n",
        "\n",
        "      # Calculate the 75th percentile\n",
        "      high_val = np.percentile(values, high_percentile)\n",
        "      low_val = np.percentile(values, low_percentile)\n",
        "\n",
        "      return (high_val, low_val)\n",
        "\n",
        "@tool\n",
        "def get_dict_values_wrapper(high_percentile: int, low_percentile: int):\n",
        "    '''\n",
        "    This is a wrapper function to embed input dictionary into the function directly and need not\n",
        "    to provide through the prompt itself\n",
        "    '''\n",
        "    return get_dict_values(input_dict, high_percentile, low_percentile)\n",
        "\n",
        "@tool\n",
        "def sample_optimization(high_val:float, low_val:float):\n",
        "      '''\n",
        "      This is an optimization function which will take two float inputs, perform the necessary\n",
        "      calculations and provide float output\n",
        "      '''\n",
        "      model = ConcreteModel()\n",
        "\n",
        "      # Define variables\n",
        "      model.x = Var(within=NonNegativeReals)\n",
        "      model.y = Var(within=NonNegativeReals)\n",
        "\n",
        "      # Define objective function: Minimize x + y\n",
        "      model.objective = Objective(expr=model.x + model.y, sense=minimize)\n",
        "\n",
        "      # Define constraints\n",
        "      model.constraint1 = Constraint(expr=model.x + 2*model.y >= int(high_val))\n",
        "      model.constraint2 = Constraint(expr=model.x - model.y <= int(low_val))\n",
        "\n",
        "      # Solve the model using GLPK solver\n",
        "      solver = pyomo.environ.SolverFactory('cbc')\n",
        "      # solver = pyomo.environ.SolverFactory('gurobi')\n",
        "      solver.solve(model)\n",
        "\n",
        "      return (value(model.objective))"
      ],
      "metadata": {
        "id": "vN7ztPK7Nmys"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model setting\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_AI_TOKEN')\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "tools = [get_dict_values_wrapper, sample_optimization]\n",
        "agent = create_tool_calling_agent(llm, tools, prompt_openai)\n",
        "\n",
        "# run\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "message = '''\n",
        "Calculate the percentil values using 'get_dict_values' function. Consider 85 as high percentile\n",
        "and 10 as low percentile value. Pass the output to sample_optimization function and\n",
        "return the final output\"\n",
        "'''\n",
        "result = agent_executor.invoke({\"input\": message})\n",
        "\n",
        "\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCkxgPoYIagy",
        "outputId": "f2a93cbc-cb8d-40ea-e894-80d508cb8398"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_dict_values_wrapper` with `{'high_percentile': 85, 'low_percentile': 10}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m(np.float64(66.0), np.float64(37.9))\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `sample_optimization` with `{'high_val': 66, 'low_val': 37.9}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m33.0\u001b[0m\u001b[32;1m\u001b[1;3mThe final output from the optimization process is **33.0**.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The final output from the optimization process is **33.0**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test run for Gemini Agent Information passdown**"
      ],
      "metadata": {
        "id": "cilyyHHeOuW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model setting\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_TOKEN')\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "tools = [get_dict_values, sample_optimization]\n",
        "agent = create_tool_calling_agent(llm, tools, prompt_gemini)\n",
        "\n",
        "# run\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "message = \"Calculate the percentil values of the provided dictionary using 'get_dict_values' function. Consider 85 as high percentile and 10 as low percentile value. Pass the output to sample_optimization function and return the final output\"\n",
        "result = agent_executor.invoke({\"input\":  message, \"input_dict\": input_dict})\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui939U9uLdjT",
        "outputId": "84368bcd-b498-4769-a6d3-85da9eee2af1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_dict_values` with `{'low_percentile': 10.0, 'input_dict': {'hour_5': 75.0, 'hour_17': 43.0, 'hour_22': 41.0, 'hour_12': 56.0, 'hour_18': 66.0, 'hour_21': 51.0, 'hour_11': 57.0, 'hour_13': 34.0, 'hour_3': 63.0, 'hour_16': 55.0, 'hour_14': 64.0, 'hour_2': 33.0, 'hour_1': 59.0, 'hour_23': 66.0, 'hour_19': 56.0, 'hour_9': 64.0, 'hour_7': 40.0, 'hour_4': 37.0, 'hour_10': 79.0, 'hour_24': 63.0, 'hour_20': 53.0, 'hour_6': 76.0, 'hour_8': 53.0, 'hour_15': 62.0}, 'high_percentile': 85.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m(np.float64(66.0), np.float64(37.9))\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `sample_optimization` with `{'low_val': 37.9, 'high_val': 66.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m33.0\u001b[0m\u001b[32;1m\u001b[1;3mThe optimization function returned 33.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The optimization function returned 33.\n"
          ]
        }
      ]
    }
  ]
}